{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4yyDD2zs-FA",
    "outputId": "7e125604-e9ad-4195-b7ea-e709256f9239"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\",\n",
    "    \"emoji\",\n",
    "    \"nltk\",\n",
    "    \"langdetect\",\n",
    "    \"swifter\"\n",
    "]\n",
    "\n",
    "# Install each package if not already installed\n",
    "for package in required_packages:\n",
    "    subprocess.call([\"pip\", \"install\", package])\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbfxP9gy0yY_",
    "outputId": "d7b61cba-2315-4d61-d885-5e2f1b1a3d3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377,
     "referenced_widgets": [
      "b415732a189f411ea89d5d31a4c610f4",
      "b18ec0d2904b41b59e80f804a42e8be2",
      "bc2059b8234e4264befbb5323652463d",
      "cbcb0a5270774335896604e95dcccf53",
      "e0dcf1d27b1d4f069c46275000c08fa2",
      "c9fc9ef89cfc4898aee952554d68f722",
      "86792e0ee23a428f9d1e2143befd82d7",
      "730e33f964ab4824ba3dc054dadb0731",
      "a8db26fe41634ab6a4f305c757e691f7",
      "347bf1ebff3649bab93703b619cfa900",
      "bc40d85aeb1e461f9a6d1b0d48f0fcad",
      "098e957b62df4987a663e7291ead8326",
      "c55cd353485f46ee92784e87fa72e563",
      "ab37d353014746e6a9f58c860dc8da94",
      "9869a963c4294945afbe873a40bb4cee",
      "9b507544454b4cc0878423102c3469ad",
      "cdbda66986eb4ebeac83340d2ef8bec6",
      "4241064a5fd345b986588266fa445050",
      "820b825035114474ba81532f0318a1da",
      "927845ecd056416a8b5ace1ee60b9b5b",
      "52bf06afd4784f17a79161e131a4aa57",
      "07a37554ee4f45ee96bb7a53c441583a",
      "777a1c8c1e4e4dabab104758ca43c911",
      "e0f092b6e9d14844abbc6802927bfe52",
      "9b6d13036eec4e59956e789540cefe1f",
      "ea0ebb34eef643b78a42f804c98f54e1",
      "d5bb2ee61ab94c7eb45f6c9d42100848",
      "7f2638e332074a2096fafad6556fa15d",
      "cb7f5d65e3bf48ff9f31f4401c0f4104",
      "0fb68eaa6b2f4ee29b30677f5eb2a0e8",
      "b53d6b59a97a4032ace6f95ddab739b5",
      "bb9f65a4c6204f43932c1ff6ec08e6a4",
      "f22a20bace174a9e9d6b8ff32f141109",
      "1da54f0841344a8188275c45f3b1a7b7",
      "cfa1b20332dc423cb3a6e43c808782d8",
      "e0282356e0734f01b83d67cf22fabe36",
      "b532bd080d5340af9088f9592daafba5",
      "37f3930ec684413583ea66675fc1473b",
      "8795f89d63ef47ea83366736d14ee4e8",
      "52a7c6c3c2e54e4fba4b3ccdb036edbb",
      "faf9383d55cb48a2b41063b47b515dd7",
      "4581134830764d399bf79455914f977a",
      "d7c22a5a73ad4858a97aec43f3c3a491",
      "728fcee3c86f4a408d020ba050041768",
      "7f6a2614e7fd4c1f9fa526ef2eeb72a4",
      "f2e3248a60d54c4597c3d3d99426e3ed",
      "00026bfa78f24cae981f5a716d02718e",
      "6b5b86ca262a418d99124432d2e969f0",
      "4e245218aefc4e36b02cc7f432061d07",
      "ab6a06c809484a1ab2581614571685ae",
      "05ad0d037d8348c1bc5922e86c3652ba",
      "995c71e4f5f241218a2ed2124dd452d1",
      "655bc7cdd3a546fbafdff63acb5cc80a",
      "5af074738c984159b463aa36b0333aa4",
      "91988497a26249359463cf4a76aea647",
      "923ab914a4494b31868c283702957cf8",
      "1759424b7dd6414a8aecc626869a4fb3",
      "844ad84027544b0e9a8261fb32ab9bab",
      "6c1d1924e2d04cb39984e7d8eda4a7fa",
      "5afceb87f6ed4181a9537268883afa07",
      "369e7aceced34f2f9b0a474ca5ae8c8f",
      "1be800947aeb4e19be684555de554173",
      "df960475b68340cf90ee02d6fa581192",
      "b5c8cb1b344540378fc15a4d036155ab",
      "c449830598b24c0faf71f19a5495db6e",
      "08896f1b4b574b59878f84c0f443cb7f",
      "15ac9d2da45646ebb07c633e7502760c",
      "06215396aa714ed0a2799ef85aefee09",
      "5639e049a82a49008269a26d19806046",
      "d03dc032130b4a36990c1b8c39434e83",
      "c690bda27ced4e85bf6bec0f88d430ae",
      "3488c073da944917b7cce1ba501c53b6",
      "5e6a056cc599487690e855987fa2c258",
      "a89933fa7e8a44e2a7f7ba20aea60358",
      "d85f4da4519a42e0bc5b75e5104ea04b",
      "dcc5153c779c4d8990bf5a8a83b5d371",
      "1cb8f0c3d1b24d31ac08d54f4044707e",
      "89fdc669661b440ab9ca8361b798d408",
      "ab94171e2a484c958ca85eef6b8f2c4a",
      "43bcf423271246ba90e47677298a009b",
      "1a73959e2d8a408c9ae51a60a8ef6233",
      "d32fb4a2052445aa8d81b001aa8e17a9",
      "9b933bcaa46c4a29b1c63dfcc79845ee",
      "bde186796f9142adb3fabb1a30b38d78",
      "462ae989914d417eaea68e3056f3883d",
      "3b222a1b017d4225a39ab30312ee736a",
      "64ee234af51d4bd08deeb9fd787145f2",
      "3b3841596f5a4ecb9945f44d2c8479fd"
     ]
    },
    "id": "duI6iyWzteuS",
    "outputId": "493c8774-e237-437a-83e8-e0dec8458310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b415732a189f411ea89d5d31a4c610f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098e957b62df4987a663e7291ead8326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777a1c8c1e4e4dabab104758ca43c911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da54f0841344a8188275c45f3b1a7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6a2614e7fd4c1f9fa526ef2eeb72a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923ab914a4494b31868c283702957cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ac9d2da45646ebb07c633e7502760c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fdc669661b440ab9ca8361b798d408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/11000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from langdetect import detect, DetectorFactory\n",
    "import swifter\n",
    "\n",
    "# Ensure deterministic language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Install necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "input_file = \"mahakumbh_dataset.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Ensure the correct column name\n",
    "column_name = \"text\"\n",
    "\n",
    "# Define collection words (modify as needed)\n",
    "collection_words = {\"example\", \"data\", \"sample\"}\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text) if len(text) >= 5 else \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Detect language once and store it\n",
    "df[\"lang\"] = df[column_name].astype(str).swifter.apply(detect_language)\n",
    "\n",
    "# --- Step 1: Remove Mentions, Hashtags, and Retweets ---\n",
    "def remove_tweets(text):\n",
    "    return re.sub(r\"@\\w+|#\\w+|RT\\s+\", \"\", text).strip()\n",
    "\n",
    "df[column_name] = df[column_name].swifter.apply(remove_tweets)\n",
    "\n",
    "# --- Step 2: Remove URLs ---\n",
    "df[column_name] = df[column_name].swifter.apply(lambda text: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text))\n",
    "\n",
    "# --- Step 3: Convert Emojis to Text ---\n",
    "df[column_name] = df[column_name].swifter.apply(lambda text: emoji.demojize(text, delimiters=(\" \", \" \")))\n",
    "\n",
    "# --- Step 4: Remove Stopwords (Before Tokenization) ---\n",
    "def remove_stopwords(text, lang):\n",
    "    if lang == \"unknown\":\n",
    "        return text\n",
    "    try:\n",
    "        stop_words = set(stopwords.words(\"english\")) if lang == \"en\" else set(stopwords.words(lang))\n",
    "        words = text.split()  # Temporary word split before tokenization\n",
    "        return \" \".join(word for word in words if word.lower() not in stop_words)\n",
    "    except:\n",
    "        return text  # Return original text if stopwords unavailable\n",
    "\n",
    "df[column_name] = df.swifter.apply(lambda row: remove_stopwords(row[column_name], row[\"lang\"]), axis=1)\n",
    "\n",
    "# --- Step 5: Stemming & Lemmatization ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def apply_stemming_lemmatization(text, lang):\n",
    "    if lang == \"unknown\":\n",
    "        return text\n",
    "    try:\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "        words = text.split()\n",
    "        return \" \".join(lemmatizer.lemmatize(stemmer.stem(word)) for word in words)\n",
    "    except:\n",
    "        return text  # Return original text if stemming fails\n",
    "\n",
    "df[column_name] = df.swifter.apply(lambda row: apply_stemming_lemmatization(row[column_name], row[\"lang\"]), axis=1)\n",
    "\n",
    "# --- Step 6: Remove Collection Words ---\n",
    "df[column_name] = df[column_name].swifter.apply(lambda text: \" \".join(word for word in text.split() if word not in collection_words))\n",
    "\n",
    "# --- Step 7: Tokenization (Moved to End) ---\n",
    "df[column_name] = df[column_name].swifter.apply(word_tokenize)\n",
    "\n",
    "# --- Save Final Preprocessed Dataset ---\n",
    "df.to_csv(\"preprocessed_mahakumbh_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O_F-0ht7mlA1",
    "outputId": "1b8542ca-3850-40c8-f396-c4628796c9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved as 'english_mahakumbh_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the final preprocessed dataset\n",
    "df = pd.read_csv(\"preprocessed_mahakumbh_data.csv\")\n",
    "\n",
    "# Keep only rows where 'lang' is 'en'\n",
    "df = df[df[\"lang\"] == \"en\"]\n",
    "\n",
    "# Save the filtered dataset\n",
    "df.to_csv(\"english_mahakumbh_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
